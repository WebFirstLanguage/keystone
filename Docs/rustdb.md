Technical Specification for a Pure-Rust Object Storage SystemI. Foundational Architecture: The Storage EngineThis section addresses the most critical decision for the entire system: the selection of a pure-Rust, embedded key-value database. The choice made here will have cascading effects on performance, reliability, transactional capabilities, and the overall complexity of the implementation. The "100% Rust" constraint is paramount and immediately disqualifies otherwise excellent but non-compliant options.1.1 The "Pure Rust" Imperative and Its ImplicationsThe primary technical constraint for this project is the requirement for a 100% pure-Rust implementation. This is a strategic architectural decision that prioritizes several key benefits:Memory Safety: Leveraging Rust's ownership and borrowing model throughout the entire stack minimizes the risk of memory-related bugs, such as buffer overflows and use-after-free errors, which are common sources of security vulnerabilities and instability.Simplified Build Process: A pure-Rust dependency graph eliminates the need for external C/C++ toolchains (like Clang and LLVM) during compilation. This simplifies the development environment setup, continuous integration pipelines, and cross-platform builds.Ecosystem Cohesion: Staying within the Rust ecosystem ensures seamless integration with other Rust libraries and tools, such as the serde serialization framework and the tokio asynchronous runtime.This constraint, however, significantly narrows the field of available storage engines. Many widely-used, high-performance embedded databases are written in C or C++ and are accessed in Rust via bindings. For example, MenhirKV is a Rust library built on RocksDB, a C++ project. This introduces the exact toolchain complexity the project seeks to avoid, as its documentation notes: "it's not pure Rust so you'll need to have clang + llvm installed, which is a consequence of having a C++ dependency".1 This sentiment is echoed in community discussions, where using RocksDB with Rust bindings is acknowledged as not being a "pure Rust" story and adding significant compile time.2Therefore, the selection process must focus exclusively on key-value stores written natively in Rust. This forces an evaluation based not only on features but on the fundamental implementation language, leading to a shortlist of mature, pure-Rust candidates that can serve as the system's physical persistence layer.1.2 Comparative Analysis of Pure-Rust Key-Value Stores: redb vs. sledThe Rust ecosystem offers several embedded databases, but two stand out as the most viable candidates for this project: redb and sled. A deep technical comparison is necessary to determine which provides the most suitable foundation.1.2.1 redb: The Stability-Focused Contenderredb is an embedded key-value store that positions itself as a simple, portable, and high-performance solution.3Core Architecture: The design of redb is "loosely inspired by lmdb" and is built upon a collection of copy-on-write B-trees.3 This is a classic, well-understood database architecture renowned for its exceptional read performance, transactional robustness, and simple, effective crash safety. Its crash recovery mechanism involves a two-phase commit where data is persisted via msync(), followed by an update to a single byte that atomically switches the active root of the B-tree, making the new data visible.4 This approach is straightforward to reason about and provides strong durability guarantees.API and Features: redb exposes a "Zero-copy, thread-safe, BTreeMap based API" that is strongly typed, allowing users to work with specific key and value types (e.g., &str to u128) rather than just byte slices.4 It offers "Fully ACID-compliant transactions" and Multi-Version Concurrency Control (MVCC), which enables concurrent readers and a single writer to operate without blocking each other.3 These features are essential for building a reliable and correct storage system.Maturity and Stability: A key philosophical pillar of the redb project is its focus on stability to achieve maturity more quickly.4 This is most evident in its commitment to a stable on-disk file format. The documentation explicitly states, "The file format is stable, and a reasonable effort will be made to provide an upgrade path if there are any future changes to it".3 The project maintainer even published a formal Request for Comments (RFC) to solicit community feedback before finalizing the format, demonstrating a serious commitment to long-term compatibility.5 For a foundational component of a new storage system, this stability is a paramount advantage.1.2.2 sled: The Performance-Oriented Innovatorsled is a modern embedded database that aims to push the boundaries of performance by leveraging advanced, lock-free data structures.6Core Architecture: sled employs a more complex and innovative architecture described as a "lock-free tree on a lock-free pagecache on a lock-free log".7 It is inspired by academic research on data structures like the Bw-Tree and uses a "flash-optimized log-structured storage" model.6 This design is analogous to a Log-Structured Merge-tree (LSM tree), which typically provides very high write throughput, while sled aims to combine this with the strong read performance of a traditional B-tree.7API and Features: Like redb, sled offers an API similar to a thread-safe BTreeMap, along with serializable ACID transactions and atomic single-key operations like compare-and-swap.6 It also includes more advanced features such as merge operators and the ability to subscribe to changes on key prefixes.7Maturity and Stability: This is the most significant drawback of sled for this project's requirements. The official documentation clearly states that sled is "beta" and "should be considered unstable for the time being".7 Critically, it includes a stark warning: "the on-disk format is going to change in ways that require manual migrations before the 1.0.0 release!".7 This lack of a forward-compatibility guarantee for the on-disk format presents an unacceptable risk for a system intended to store user data persistently. While sled is used as the backend for the kv crate 8, that crate itself has not been updated in several years, suggesting a potential lack of momentum for its use in production-stable systems.91.2.3 Other CandidatesOther pure-Rust libraries exist but are less suitable. For instance, agatedb is explicitly described as being in "early heavy development" and experimental.10native_db appears to be a higher-level library focused on directly mapping Rust structs to storage, rather than a general-purpose key-value engine upon which a more complex system can be built.111.3 Architectural RecommendationRecommendation: redb is the unequivocally superior choice for this project's foundational storage engine.The choice of a storage engine is not merely a technical decision but a fundamental risk management strategy. The primary risk at this foundational stage is building upon an unstable component that could necessitate a complete rewrite or a complex and error-prone data migration process in the future.The justification for selecting redb is grounded in the following critical factors:On-Disk Format Stability: This is the single most important criterion. redb's explicit commitment to a stable file format provides the long-term assurance necessary for a persistent datastore.3 Data written with the initial version of the object storage system will be readable by future versions. sled's transparent admission that its format is unstable and will require manual migrations is a disqualifying risk.7Architectural Predictability: The B-tree architecture used by redb is proven, and its performance characteristics are well-understood and predictable, particularly its strong read performance.3 While sled's log-structured design may offer higher write throughput in some scenarios, it also introduces operational complexities such as garbage collection (compaction) and potential for high space amplification, an issue the sled project acknowledges ("sled uses too much space sometimes").7 For a V1 system focused on correctness and stability, the simpler, more predictable model is preferable.Alignment with Project Goals: The user's directive is to "hash out" the datastore first, implying a need for a reliable, robust, and stable foundation. redb's design philosophy and stable status align perfectly with this goal. sled is positioned as a more experimental, performance-focused engine, making it better suited for applications where durability and format stability are secondary to raw throughput, such as caching.Furthermore, the "100% Rust" constraint ties the project's long-term health directly to the maturity of its dependencies. The contrast between redb's stability-first approach and sled's experimental nature illustrates a maturing, but not yet fully mature, ecosystem. This reality dictates a defensive architectural posture. The system design must include a dedicated storage abstraction layer, insulating the core object-storage logic from the specific implementation details of redb. This ensures that if a superior pure-Rust engine emerges in the future, it can be integrated with minimal disruption to the rest of the application.FeatureredbsledJustification & Source ReferencesCore Data StructureCopy-on-Write B-TreeBw-Tree / Log-Structured Storageredb uses a classic, read-optimized structure.3sled uses a modern, write-optimized, log-structured design.6On-Disk Format StabilityStableUnstableredb's format is explicitly stable and finalized.3sled's format is explicitly unstable and will change, requiring manual migrations.7Transactional GuaranteesFully ACID, MVCCSerializable ACID TransactionsBoth provide strong, modern transactional guarantees suitable for this project.3API StyleStrongly Typed BTreeMap<K, V>BTreeMap<[u8], [u8]>-likeredb offers a more type-safe API out of the box 4, while sled's API is byte-oriented, leaving typing to the user.Concurrency ModelMVCC (non-blocking readers)Lock-free algorithmsredb uses MVCC for concurrent reads and a single writer.3sled is built on lock-free principles for high concurrency.7Project Maturity/StatusStable and MaintainedBeta, Rewrite in Progressredb is considered stable for production use.3sled is explicitly beta, with a major storage rewrite underway.7Primary Trade-offSimplicity, Read PerformanceWrite Performance, Complexityredb prioritizes stability and read speed. sled prioritizes write speed and concurrency at the cost of complexity and stability.II. Data Modeling and Persistence StrategyThis section defines the logical data model—comprising buckets, keys, and objects—and specifies the precise strategy for mapping this hierarchical model onto the flat, key-value physical layer provided by redb. This translation is a non-trivial engineering task that forms the core of the system's internal logic and dictates the performance characteristics of key operations.2.1 Mapping the Hierarchical Model to a Flat Key-SpaceThe user-facing model of an object store presents a two-level hierarchy: an object is identified by a key within a containing bucket. However, the underlying redb database is a simple, flat key-value store, effectively a single, massive, sorted map. The software's primary role is to manage the tension between these two models. A robust and efficient mapping scheme is therefore required.Proposed Solution: Key PrefixingA consistent, parsable key structure will be used to encode the hierarchy into a flat key. The proposed format is:<bucket_name>\0<object_key>The null byte (\0) is chosen as a separator because it is a control character that is typically disallowed in user-provided bucket and object names, thus ensuring an unambiguous boundary for parsing. This prefixing scheme is the bridge between the logical model and the physical storage.This design has profound implications for API operations. For example, listing all objects within a bucket is no longer a simple "read directory" operation. Instead, it becomes a database range scan over all keys starting with the prefix <bucket_name>\0. The performance of this common operation is therefore directly tied to the efficiency of the B-Tree's prefix scanning capabilities.6Bucket Metadata ManagementBeyond storing objects, the system must also track the existence and metadata of the buckets themselves (e.g., creation timestamp). To manage this, a separate, reserved key prefix will be used to create a distinct namespace for metadata. This prevents collisions between user data and system data. The proposed format for bucket metadata keys is:__meta__\0bucket\0<bucket_name>Storing a value (even an empty one) at this key signifies the existence of the bucket. Deleting a bucket, therefore, becomes a complex transaction: the system must first perform a range scan to find and delete all object keys with the bucket's prefix and then delete the bucket's metadata key. This approach is validated by existing libraries like kv, which builds on sled and explicitly introduces the concept of a Bucket to provide "typed access to a section of the key/value store," demonstrating the pattern of layering a bucket abstraction over a raw KV store.82.2 Object Serialization and TypingThe system must accommodate arbitrary data, as an object can be "anything from a file to a simple string, FLOAT, integer or boolean" (User Query). This requires a generic and efficient serialization strategy. A generic object store is not merely a data store; it is a system for type erasure and reconstruction.The Serde FrameworkSerde is the de-facto standard for serialization and deserialization in the Rust ecosystem, valued for its performance, flexibility, and broad format support.1 Its trait-based design allows for the decoupling of data structures from their serialized representation.15 We will leverage Serde by defining our core data structures with # and requiring that any user-provided data also implement these traits.Serialization Format: bincodeFor a high-performance system storing arbitrary data, a binary serialization format is vastly more efficient in both space and processing time than text-based formats like JSON. bincode is a crate that provides a compact, high-performance binary format that integrates seamlessly with Serde.1 The output of serializing a Rust type with bincode is a Vec<u8>, which can be stored directly as the value in our redb database.Data Structure DefinitionWhen an object is stored, its Rust type (e.g., i32, String, MyStruct) is serialized into a byte vector (Vec<u8>). At the storage layer, all data is just bytes; the original type information is erased. To reconstruct the object upon retrieval, the user must specify the expected type. This reality dictates that the API for retrieving data must be generic.The following conceptual structs define the data that will be serialized and stored as the value in the key-value store:Rustuse serde::{Serialize, Deserialize};
use std::time::{SystemTime, UNIX_EPOCH};

// The generic container for any user data that is serializable.
#
pub struct Object<T> {
    pub metadata: ObjectMetadata,
    pub data: T,
}

// Metadata stored alongside every object.
#
pub struct ObjectMetadata {
    /// The size of the serialized `data` field in bytes.
    pub size: u64,
    /// A user-provided content type string (e.g., "application/json", "image/png").
    pub content_type: String,
    /// The timestamp of the last modification, stored as seconds since the UNIX epoch.
    pub last_modified: u64,
    // Additional metadata fields (e.g., checksums) can be added here.
}
This structure ensures that essential metadata is always stored atomically with the object's data. The content_type field provides a hint to the user about how to interpret the data, even though the storage system itself treats it as an opaque blob.2.3 Strategy for Large Object ManagementStoring multi-gigabyte files as a single value in a key-value store is highly impractical. This approach would require allocating a massive contiguous byte vector in memory for both serialization and deserialization, leading to extreme memory pressure and likely application failure. Furthermore, it prevents streaming operations and makes partial updates or reads impossible.16Proposed Solution: Chunking and Multipart OperationsTo efficiently handle large objects, the system will implement a chunking strategy analogous to Amazon S3's Multipart Upload functionality. This approach is a standard pattern for any serious object storage system, as evidenced by its inclusion in the object_store crate's API via the put_multipart method.19 The AWS best practices for its own key-value stores also explicitly recommend breaking up large items into multiple smaller items.17The process will be as follows:Manifest Object: When a large object is stored, a special "manifest" object is written to the primary user-facing key (<bucket_name>\0<object_key>). This manifest does not contain the object data itself. Instead, it stores the object's metadata and an ordered list of unique identifiers for the data chunks.Data Chunks: The actual object data is split into smaller, fixed-size chunks (e.g., 8 MB). Each chunk is stored as a separate key-value pair in the database.Chunk Keys: The keys for these data chunks will be derived from the main object key to ensure they are colocated in the B-tree and to prevent namespace collisions. A proposed format is:__data__\0<bucket_name>\0<object_key>\0<chunk_part_number>This design offers numerous advantages:Streaming I/O: Objects can be read and written in a streaming fashion, chunk by chunk, keeping memory usage low and constant regardless of the total object size.Resumability: Failed uploads or downloads can be resumed from the last successfully transferred chunk.Parallelism: Chunks can potentially be uploaded or downloaded in parallel to saturate network or disk bandwidth.Efficient Updates: For data formats that support it, updating a portion of a large object might only require re-writing a few chunks instead of the entire object.III. API Design: A Coherent and Ergonomic InterfaceThis section defines the public Application Programming Interface (API) for the library. The design is guided by the principles of clarity, type safety, and idiomatic Rust. It draws inspiration from established patterns in the Rust storage ecosystem to ensure the interface is powerful, predictable, and intuitive for developers to use.3.1 Core API AbstractionsThe API is structured around two primary entry points, Datastore and Bucket, to provide a logical and scoped way of interacting with the storage system. This two-tiered structure is a common pattern seen in libraries like kv and object_store, which use similar concepts to manage different sections of an underlying store.8DatastoreThe Datastore struct is the main entry point for the entire storage system. It is responsible for managing the connection to the underlying redb::Database file on disk.Datastore::new(path: P) -> Result<Self, Error> where P: AsRef<Path>: Opens an existing database file at the specified path or creates a new one if it does not exist.Datastore::create_bucket(name: &str) -> Result<(), Error>: Creates a new, empty bucket within the datastore. This operation is atomic.Datastore::delete_bucket(name: &str) -> Result<(), Error>: Deletes an existing bucket and all objects contained within it. This is a potentially long-running but fully atomic transaction.Datastore::bucket(name: &str) -> Result<Bucket, Error>: Returns a Bucket handle, which is a lightweight struct used to perform operations within the scope of a specific bucket. This call verifies that the bucket exists.BucketThe Bucket struct is a handle to a specific bucket. All object-level operations are performed through this struct, ensuring that operations are always scoped to the correct container.Bucket::put<T: Serialize>(key: &str, data: &T) -> Result<ObjectMetadata, Error>: Serializes and stores an object with the given key. If an object with the same key already exists, it will be overwritten. Returns metadata about the stored object.Bucket::get<T: for<'de> Deserialize<'de>>(key: &str) -> Result<T, Error>: Retrieves an object by its key and deserializes it into the specified type T.Bucket::delete(key: &str) -> Result<(), Error>: Deletes an object by its key.Bucket::list(prefix: &str) -> Result<impl Iterator<Item = Result<ObjectMetadata, Error>>, Error>: Returns an iterator over the metadata of all objects in the bucket whose keys begin with the given prefix.The design of this API is a direct reflection of the system's underlying capabilities and constraints. The existence of Bucket::list is a consequence of the key-prefixing data model, which makes prefix scans efficient. Conversely, the absence of a global Datastore::search_all_objects method is an intentional design choice to prevent users from accidentally triggering a full-database scan, which would be prohibitively expensive. The generic nature of the get<T> method is a direct result of the type-erasing serialization strategy. Every element of the public API is a deliberate choice dictated by the foundational decisions made in the architecture and data modeling sections.3.2 Transactional Guarantees and AtomicityThe choice of redb as the backend provides the system with fully ACID-compliant write transactions, a powerful feature that will be exposed through the API to enable complex, atomic operations.3Implicit Single-Operation Atomicity: Every individual mutating operation (put, delete, create_bucket, delete_bucket) will be executed within its own dedicated redb write transaction. This guarantees that each operation is atomic (it either completes fully or has no effect), consistent (it leaves the database in a valid state), isolated (it does not interfere with other concurrent operations), and durable (once it completes, the changes are permanent).Explicit Multi-Operation Transactions: To support use cases that require multiple operations to succeed or fail as a single atomic unit (e.g., moving an object by copying it to a new key and deleting the old one), the API will provide a closure-based transaction mechanism. This is a common and safe pattern in Rust for managing stateful operations.Rust// Conceptual API for an atomic transaction
datastore.transaction(|tx| {
    let bucket1 = tx.bucket("orders")?;
    let order = Order { id: 123, status: "processing" };
    bucket1.put("order-123", &order)?;

    let bucket2 = tx.bucket("inventory")?;
    bucket2.decrement_stock("item-abc", 1)?;

    // The transaction is automatically committed if the closure returns Ok.
    Ok(())
})?;
// If the closure returns an Err, the transaction is automatically aborted.
This API provides callers with direct access to the underlying database's transactional power in a safe and ergonomic way.3.3 Comprehensive Library-Grade Error HandlingA library's error handling strategy is a core part of its user interface and public contract. For a system library, it is not sufficient to panic on errors or return a generic error type. Consumers of the library must be able to programmatically inspect and react to different failure modes.thiserror for Structured, Typed Errors: The library will define a single, comprehensive public Error enum using the thiserror crate.20 This approach allows for the creation of distinct, well-documented error variants that a calling application can handle with a match statement. For example, a caller can distinguish between Error::BucketNotFound (which might be a recoverable condition) and Error::StorageError (which might be a fatal one).Contrast with anyhow: While crates like anyhow are excellent for application-level error handling where the primary goal is to add context and report the error to a user or log file, they are less suitable for libraries.20anyhow works by erasing the specific error type into a generic error object, which prevents the library's consumer from programmatically handling different error cases. By providing specific error variants, the library gives its consumer the power and responsibility to handle those cases intelligently.Proposed Public Error Enum:Rustuse thiserror::Error;

#
pub enum Error {
    #
    BucketNotFound(String),

    #[error("Object not found: {key} in bucket {bucket}")]
    ObjectNotFound { bucket: String, key: String },

    #
    BucketAlreadyExists(String),

    #[error("Invalid bucket name: {0}")]
    InvalidBucketName(String),

    #[error("Invalid object key: {0}")]
    InvalidObjectKey(String),

    #[error("Underlying database error: {0}")]
    StorageError(#[from] redb::Error),

    #
    SerializationError(#[from] bincode::Error),

    #[error("I/O error during operation: {0}")]
    IoError(#[from] std::io::Error),
}
The #[from] attribute provided by thiserror is particularly powerful. It automatically generates From implementations, allowing underlying errors from dependencies like redb, bincode, and std::io to be converted into our library's Error type seamlessly using the ? operator.20 This keeps the implementation code clean and idiomatic while providing rich, typed errors to the user.StructMethod SignatureDescriptionReturnsPossible ErrorsDatastorepub fn new<P: AsRef<Path>>(path: P) -> Result<Self, Error>Opens or creates a datastore file at the given path.Ok(Self)Error::StorageError, Error::IoErrorpub fn create_bucket(name: &str) -> Result<(), Error>Creates a new, empty bucket.Ok(())Error::BucketAlreadyExists, Error::InvalidBucketName, Error::StorageErrorpub fn delete_bucket(name: &str) -> Result<(), Error>Deletes a bucket and all its contents atomically.Ok(())Error::BucketNotFound, Error::StorageErrorpub fn bucket(name: &str) -> Result<Bucket, Error>Gets a handle to an existing bucket for object operations.Ok(Bucket)Error::BucketNotFound, Error::StorageErrorpub fn transaction<F, T>(&self, f: F) -> Result<T, Error> where F: FnOnce(&mut Transaction) -> Result<T, Error>Executes a closure within a single atomic transaction.Ok(T)Any Error variant returned by the closure or Error::StorageError.Bucketpub fn put<T: Serialize>(&self, key: &str, data: &T) -> Result<ObjectMetadata, Error>Creates or overwrites an object in the bucket.Ok(ObjectMetadata)Error::InvalidObjectKey, Error::SerializationError, Error::StorageErrorpub fn get<T: for<'de> Deserialize<'de>>(&self, key: &str) -> Result<T, Error>Retrieves and deserializes an object from the bucket.Ok(T)Error::ObjectNotFound, Error::SerializationError, Error::StorageErrorpub fn delete(&self, key: &str) -> Result<(), Error>Deletes an object from the bucket.Ok(())Error::ObjectNotFound, Error::StorageErrorpub fn list(&self, prefix: &str) -> Result<impl Iterator<Item = Result<ObjectMetadata, Error>>, Error>Lists metadata for objects with a matching key prefix.Ok(Iterator)Error::StorageErrorIV. A Test-Driven Development (TDD) BlueprintThis section provides a concrete, actionable plan for implementing the system using a strict Test-Driven Development (TDD) methodology. This approach ensures that every line of production code is written in direct response to a failing test, which guarantees high test coverage and results in a design that is inherently and demonstrably correct and testable.4.1 The TDD Philosophy and CycleThe development process will rigorously adhere to the classic "Red-Green-Refactor" cycle, a core tenet of TDD.24Red: Write a new test for a piece of desired functionality. This test must fail to compile or run, because the corresponding implementation code does not yet exist. This step defines the requirement.Green: Write the absolute minimum amount of production code required to make the failing test pass. The goal here is not elegance or optimization, but simply correctness as defined by the test.Refactor: With the safety net of a passing test, improve the design of the newly written code. This includes removing duplication, improving clarity, and enhancing performance, all while ensuring that the entire test suite continues to pass.This iterative cycle fundamentally inverts the traditional relationship between code and its specification.25 The test suite itself becomes the living, executable specification for the system. Production code is written for the sole purpose of satisfying this specification, forcing a consumer-first design perspective and ensuring that the final API contract, as defined in the previous section, is continuously verified.4.2 Project and Test StructureThe project's structure will be organized to clearly separate different types of tests, a standard practice in the Rust ecosystem.26Unit Tests: These tests will be located within a mod tests block annotated with #[cfg(test)] inside each .rs file containing the logic they are testing.26 Unit tests focus on a single function or module in isolation. Dependencies on external components, like the database, will be replaced with mocks or test doubles to ensure the test is fast and focused purely on the business logic.Integration Tests: These tests will reside in a separate /tests directory at the root of the crate. Each file in this directory is compiled as a separate test crate that links against our main library. Integration tests are designed to test the public API of the library end-to-end, verifying that all the individual components work together correctly.26 For this project, they will interact with a real redb database instance created in a temporary directory.Test Setup and Teardown: To ensure that integration tests are isolated and repeatable, each test will create its own database in a temporary directory using a crate like tempfile. This directory and its contents will be automatically deleted when the test completes, regardless of whether it passed or failed.This layered testing strategy naturally promotes a decoupled architecture. The need to write isolated unit tests for business logic (like key generation) forces a separation of that pure logic from the side-effect-ful database interactions. This organically pushes the design towards well-regarded architectural patterns like Hexagonal Architecture or Ports and Adapters, where the core application logic is independent of external dependencies.27 The integration tests then serve to verify that these decoupled layers are wired together correctly.4.3 The TDD Workflow in Practice: Implementing Bucket::put and Bucket::getThe following narrative demonstrates the practical application of the TDD cycle for implementing the core object storage functionality.Step 1 (Red - API Layer Integration Test)Begin by writing a failing integration test in /tests/object_operations.rs. This test defines the desired end-to-end behavior from the perspective of a library user.Rust// in /tests/object_operations.rs
use my_object_store::{Datastore, Error};
use serde::{Serialize, Deserialize};
use tempfile::tempdir;

#
struct User {
    id: u32,
    username: String,
}

#[test]
fn test_put_get_roundtrip() -> Result<(), Error> {
    let dir = tempdir().unwrap();
    let datastore = Datastore::new(dir.path())?;

    datastore.create_bucket("users")?;
    let bucket = datastore.bucket("users")?;

    let user_data = User { id: 42, username: "alice".to_string() };
    bucket.put("user-42", &user_data)?;

    let retrieved_user: User = bucket.get("user-42")?;
    assert_eq!(user_data, retrieved_user);

    Ok(())
}
This test will fail to compile because the Datastore, Bucket, and Error types, and their associated methods, do not exist.Step 2 (Green - API Skeletons)Create the minimal public structs and empty or unimplemented!() function skeletons in src/lib.rs to satisfy the compiler. The test will now compile but will fail at runtime when it hits an unimplemented! macro.Step 3 (Red/Green - Unit Test for Logic)Drill down to a smaller piece of logic. The put method needs to generate a composite key. Write a unit test for this specific function.Rust// in src/datastore.rs's `mod tests`
#[test]
fn test_composite_key_generation() {
    let key = generate_object_key("users", "user-42");
    assert_eq!(key, b"users\0user-42");
}
This test fails. Write the generate_object_key function to make it pass.Step 4 (Red - Integration Test Progress)The integration test still fails. Now, focus on the database interaction within the put method. The code needs to serialize the data and insert it into a redb table.Step 5 (Green - Full put Implementation)Implement the Bucket::put method. It will:Begin a redb write transaction.Open the main objects table.Generate the composite key using the now-tested function.Create an Object instance containing the user data and metadata.Serialize the Object using bincode.Insert the serialized bytes into the table using the composite key.Commit the transaction.After this step, the put call in the integration test will succeed, but the test will now fail on the get call.Step 6 (Repeat for get)Repeat the TDD cycle for the get method.Red: The integration test already provides the failing case.Green: Implement Bucket::get. It will perform the inverse operations: start a read transaction, generate the key, look up the key in the table, and if found, deserialize the bytes back into the requested type T.Red/Green (Error Handling): Write a new integration test that attempts to get a non-existent key and asserts that it returns Error::ObjectNotFound. Implement this error-handling logic.Step 7 (Refactor)Once the test_put_get_roundtrip integration test passes completely, review the new code.Is the key generation logic duplicated? Extract it into a shared private module.Is the transaction management code clean?Are the error conversions from redb::Error and bincode::Error handled idiomatically with ?Continuously run the test suite during refactoring to ensure no regressions are introduced.4.4 Testing Strategy for Critical ComponentsBeyond the main workflow, specific strategies will be employed for critical and complex parts of the system.Error Conditions: Every public function will have dedicated tests to verify that it returns the correct Error variant under specific failure conditions (e.g., trying to create a bucket that already exists, providing an invalid key). The assert!(result.is_err()) pattern will be used, followed by a match to confirm the specific error type. For internal invariants that should never be violated, #[should_panic] will be used to test that the code correctly panics in these exceptional circumstances.26Serialization: Tests will be written to ensure that complex, nested data structures, as well as edge cases like empty collections, Option<T> variants, and various primitive types, can be successfully round-tripped through the serialization and deserialization process.Large Objects: Integration tests will be created to specifically exercise the chunking mechanism. These tests will write multi-megabyte objects (e.g., 100 MB) and then read them back, verifying byte-for-byte integrity. These tests will also monitor memory usage to ensure it remains low and constant, proving the effectiveness of the streaming approach.Concurrency: Although redb provides its own concurrency guarantees via MVCC, the application logic built on top must also be thread-safe. Integration tests will be written that use std::thread::spawn or tokio::spawn to perform many simultaneous reads while a separate thread performs writes, ensuring that the system remains consistent and does not deadlock.V. Implementation Roadmap and Future ConsiderationsThis final section outlines a phased implementation plan based on the TDD blueprint and provides architectural guidance for future enhancements. The goal is to ensure that the Version 1 design is not only robust and correct but also serves as a solid, extensible foundation for subsequent development.5.1 Phased Implementation PlanThe project will be implemented in three distinct, sequential phases. This inside-out approach, building from the storage core outwards to the public API, is a powerful project de-risking strategy. It ensures that each layer is fully tested and validated before the next layer is built upon it, minimizing complex integration issues and simplifying debugging.Phase 1: The Storage Abstraction Layer (SAL)Goal: To create a clean internal boundary between the object storage logic and the specific implementation details of the chosen key-value store (redb).Tasks:Define a private Rust trait, KeyValueStore, that specifies the essential operations required from a backend: get(key), put(key, value), delete(key), scan_prefix(prefix), and methods for transaction management.Implement this KeyValueStore trait for redb. This implementation, the "adapter," will be the only part of the system that directly depends on the redb crate.TDD Focus: This phase will be driven by unit tests that validate the redb adapter. For example, a test will call the adapter's put method and then use the raw redb API to verify that the correct key and value were written to the database.Phase 2: Core Data Model and LogicGoal: To implement the core business logic of the object store, including the bucket/key mapping, object serialization, and the mechanism for handling large objects.Tasks:Implement the Object and ObjectMetadata structs.Implement the key-prefixing logic for both objects and bucket metadata.Implement the chunking strategy for large objects, including the logic for creating and parsing manifest objects.Crucially, this entire layer of logic will be written to depend only on the KeyValueStore trait, not on redb directly.TDD Focus: This phase will be driven by unit tests. A mock implementation of the KeyValueStore trait will be created (e.g., using an in-memory HashMap). This allows the complex logic of serialization and chunking to be tested quickly and in complete isolation from the actual database, dramatically speeding up the development cycle.Phase 3: The Public APIGoal: To implement the final, public-facing Datastore and Bucket structs and their methods, as defined in the API specification (Table 2).Tasks:Implement the methods on Datastore and Bucket. These methods will act as the "glue," translating the public API calls into operations on the core logic layer from Phase 2.Implement the comprehensive, thiserror-based public Error enum, ensuring that all internal errors are correctly mapped to the appropriate public variants.TDD Focus: This phase is driven almost exclusively by the integration tests located in the /tests directory. These tests validate the entire system end-to-end, from the public API call down to the physical write in the redb database file, ensuring all layers are correctly integrated.5.2 Pathways to Version 2: Architectural ExtensibilityGood architecture is about drawing the right boundaries to accommodate future change. The Version 1 design is deliberately structured to ensure that future enhancements like networking, authentication, and alternative storage backends can be added without requiring a fundamental redesign of the core system.Networking (API Layer)The embedded library created in V1 is the core engine. To expose it over a network, it can be wrapped by a web service framework like axum or tokio. A new binary crate would be created that uses the V1 library and starts a web server. The principles of RESTful API design can be applied, mapping HTTP verbs (GET, PUT, DELETE) directly to the library's get, put, and delete functions.28 Buckets and objects are natural candidates for REST resources. This clean separation ensures the core storage logic remains independent of the network protocol.Authentication and AuthorizationSecurity concerns should be handled at the boundary of the system—in the networking layer. Authentication (verifying identity) and authorization (checking permissions) can be implemented as middleware within the web framework. This middleware would process incoming requests, validate credentials (e.g., API keys, JWTs), and then, only if successful, pass the request on to the core storage library. The library itself remains blissfully unaware of users, roles, or permissions, adhering to the principle of separation of concerns. This approach is analogous to the Backends for Frontends (BFF) pattern, where a generic core service is fronted by an interface that handles user-specific concerns.30Pluggable Storage BackendsThe Storage Abstraction Layer (SAL) created in Phase 1 is the key to long-term architectural flexibility. While the initial implementation targets redb, the SAL makes the backend pluggable. This design mirrors that of mature libraries like object_store, which supports numerous backends such as the local filesystem, Amazon S3, and Google Cloud Storage through a common trait.19 In the future, new adapters could be written to implement the KeyValueStore trait for other backends without altering a single line of the core object storage logic:An in-memory backend: Useful for high-speed testing or ephemeral storage use cases.A distributed backend: An adapter could be written for a distributed key-value store like TiKV (which is also written in Rust and is a backend option for SurrealDB 32), allowing the single-node object store to scale out horizontally.The architectural boundaries drawn in Version 1—between the storage implementation and the core logic, and between the core logic and the user-facing API—are deliberately placed at the points where change is most anticipated. This ensures that the work done today provides a stable, not brittle, foundation for the work of tomorrow.VI. ConclusionThis specification outlines a comprehensive and robust design for a pure-Rust, bucket-key-object storage system. By adhering to the principles of Test-Driven Development and making deliberate, well-justified architectural choices, the proposed plan provides a clear path to creating a high-quality, reliable, and extensible library.The key architectural decisions are:Selection of redb as the Storage Engine: Prioritizing its stable on-disk format, predictable B-tree architecture, and ACID compliance provides a rock-solid foundation, mitigating the long-term risks associated with data format instability.A Layered, Decoupled Architecture: The separation of concerns into a Storage Abstraction Layer, a Core Logic Layer, and a Public API Layer ensures the system is modular, testable, and extensible. This design is not merely for V1 but is explicitly crafted to accommodate future growth into a networked, multi-backend system.A Rigorous TDD Methodology: By making the test suite the executable specification for the system, TDD guarantees high code quality, correctness, and a design that is inherently consumer-focused and easy to maintain.Library-Grade API and Error Handling: The proposed public API is designed to be ergonomic and idiomatic for Rust developers, while the thiserror-based error handling provides the necessary control and clarity for consumers of the library.By following this specification, the resulting implementation will be a performant, memory-safe, and reliable local datastore that fully satisfies the initial project requirements while being strategically positioned for future evolution.